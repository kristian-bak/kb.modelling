% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/f_train.R
\name{f_train}
\alias{f_train}
\title{Training xgboost models}
\usage{
f_train(
  df_train,
  df_test,
  target,
  var,
  nrounds = 32,
  max.depth = 4,
  eta = 1,
  min_child_weight = 50,
  early_stopping_rounds = 25,
  subsample = 0.9,
  colsample_bytree = 0.9,
  gamma = 1,
  ...
)
}
\arguments{
\item{df_train}{train data. Should be data.table object}

\item{df_test}{test data. Should be data.table object}

\item{target}{target variable. Should be a string with a variable present in train data.}

\item{var}{variables used to train the model. Should be a character vector}

\item{nrounds}{max number of iterations. Default is 32}

\item{max.depth}{Max depth of the trees, i.e. the number of splits made.}

\item{eta}{step size of each boosting step. Large eta may lead to unstable results. Default is 1.}

\item{min_child_weight}{minimum counts in a child. The algorithm stops if splitting leads to leaf node with fewer than min_child_weight instances.}

\item{early_stopping_rounds}{stopping criteria. If performance is not improved after k(= early_stopping_rounds) iterations, the algorithm stops.}

\item{subsample}{train proportion for each fold in cross validation. Should be a number between 0 and 1.}

\item{colsample_bytree}{subsample proportion for each tree. Should be a number between 0 and 1.}

\item{gamma}{minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.}

\item{...}{additional parameters passed to xgboost.}
}
\value{
xgboost object
}
\description{
Training xgboost models
}
